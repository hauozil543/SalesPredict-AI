{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ae63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d094587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Định nghĩa hàm reduce_mem_usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a75f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Mem. usage decreased to 95.00 Mb (78.7% reduction)\n",
      "Sales train validation has 30490 rows and 1919 columns\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Đọc dữ liệu\n",
    "def read_data():\n",
    "    INPUT_DIR_PATH = 'C:/Users/Ho Hau/Downloads/M5/data/raw/'\n",
    "    sell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\n",
    "    sell_prices_df = reduce_mem_usage(sell_prices_df)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n",
    "    \n",
    "    calendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\n",
    "    calendar_df = reduce_mem_usage(calendar_df)\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n",
    "    \n",
    "    sales_train_validation_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_validation.csv')\n",
    "    sales_train_validation_df = reduce_mem_usage(sales_train_validation_df)\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation_df.shape[0], sales_train_validation_df.shape[1]))\n",
    "    \n",
    "    return sell_prices_df, calendar_df, sales_train_validation_df\n",
    "\n",
    "sell_prices_df, calendar_df, sales_train_validation_df = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fceef38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết nối đến SQLite\n",
    "conn = sqlite3.connect('database/m5_forecasting.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acd8cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar_df.to_sql('calendar', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a59ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sell_prices to SQLite...\n",
      "Processing sell_prices chunk 1/14\n",
      "Processing sell_prices chunk 2/14\n",
      "Processing sell_prices chunk 3/14\n",
      "Processing sell_prices chunk 4/14\n",
      "Processing sell_prices chunk 5/14\n",
      "Processing sell_prices chunk 6/14\n",
      "Processing sell_prices chunk 7/14\n",
      "Processing sell_prices chunk 8/14\n",
      "Processing sell_prices chunk 9/14\n",
      "Processing sell_prices chunk 10/14\n",
      "Processing sell_prices chunk 11/14\n",
      "Processing sell_prices chunk 12/14\n",
      "Processing sell_prices chunk 13/14\n",
      "Processing sell_prices chunk 14/14\n"
     ]
    }
   ],
   "source": [
    "# Đọc và lưu sells_price \n",
    "print(\"Saving sell_prices to SQLite...\")\n",
    "chunk_size = 500000  # Số hàng mỗi chunk\n",
    "num_chunks = len(sell_prices_df) // chunk_size + (1 if len(sell_prices_df) % chunk_size else 0)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"Processing sell_prices chunk {i+1}/{num_chunks}\")\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(sell_prices_df))\n",
    "    chunk = sell_prices_df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    if i == 0:\n",
    "        chunk.to_sql('sell_prices', conn, if_exists='replace', index=False)\n",
    "    else:\n",
    "        chunk.to_sql('sell_prices', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d88c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sales_train to SQLite...\n",
      "Processing sales_train chunk 1/7\n",
      "Processing sales_train chunk 2/7\n",
      "Processing sales_train chunk 3/7\n",
      "Processing sales_train chunk 4/7\n",
      "Processing sales_train chunk 5/7\n",
      "Processing sales_train chunk 6/7\n",
      "Processing sales_train chunk 7/7\n"
     ]
    }
   ],
   "source": [
    "# Đọc và lưu sales_train (sales_train_validation_df đã có sẵn)\n",
    "print(\"Saving sales_train to SQLite...\")\n",
    "chunk_size = 5000  # Số hàng mỗi chunk\n",
    "num_chunks = len(sales_train_validation_df) // chunk_size + (1 if len(sales_train_validation_df) % chunk_size else 0)\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"Processing sales_train chunk {i+1}/{num_chunks}\")\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(sales_train_validation_df))\n",
    "    chunk = sales_train_validation_df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    if i == 0:\n",
    "        chunk.to_sql('sales_train', conn, if_exists='replace', index=False)\n",
    "    else:\n",
    "        chunk.to_sql('sales_train', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d505070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating days table...\n"
     ]
    }
   ],
   "source": [
    "# Tạo bảng days chứa các số từ 1 đến 1913\n",
    "print(\"Creating days table...\")\n",
    "cursor.execute('DROP TABLE IF EXISTS days')\n",
    "cursor.execute('CREATE TABLE days (day_num INTEGER)')\n",
    "days = [(i,) for i in range(1, 1914)]\n",
    "cursor.executemany('INSERT INTO days (day_num) VALUES (?)', days)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8ebc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sales_melted table...\n"
     ]
    }
   ],
   "source": [
    "# Tạo bảng sales_melted bằng SQL\n",
    "print(\"Creating sales_melted table...\")\n",
    "cursor.execute('DROP TABLE IF EXISTS sales_melted')\n",
    "# Tạo câu lệnh CASE cho tất cả 1913 ngày\n",
    "case_statements = '\\n'.join([f\"WHEN day_num = {i} THEN d_{i}\" for i in range(1, 1914)])\n",
    "query = f'''\n",
    "CREATE TABLE sales_melted AS\n",
    "SELECT \n",
    "    s.item_id, s.dept_id, s.cat_id, s.store_id, s.state_id,\n",
    "    'd_' || d.day_num AS d,\n",
    "    CASE \n",
    "        {case_statements}\n",
    "    END AS sales\n",
    "FROM sales_train s\n",
    "CROSS JOIN days d\n",
    "WHERE sales IS NOT NULL\n",
    "'''\n",
    "cursor.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6741ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sales_data table...\n"
     ]
    }
   ],
   "source": [
    "# Tạo bảng sales_data bằng SQL\n",
    "print(\"Creating sales_data table...\")\n",
    "cursor.execute('DROP TABLE IF EXISTS sales_data')\n",
    "cursor.execute('''\n",
    "CREATE TABLE sales_data AS\n",
    "SELECT \n",
    "    s.item_id, s.dept_id, s.cat_id, s.store_id, s.state_id,\n",
    "    s.d, s.sales,\n",
    "    c.date, c.wm_yr_wk, c.weekday, c.wday, c.month, c.year,\n",
    "    c.snap_CA, c.snap_TX, c.snap_WI,\n",
    "    p.sell_price,\n",
    "    CASE WHEN c.event_name_1 IS NULL THEN 'None' ELSE c.event_name_1 END AS event_name_1,\n",
    "    CASE WHEN c.event_name_2 IS NULL THEN 'None' ELSE c.event_name_2 END AS event_name_2\n",
    "FROM sales_melted s\n",
    "LEFT JOIN calendar c ON s.d = c.d\n",
    "LEFT JOIN sell_prices p ON s.store_id = p.store_id AND s.item_id = p.item_id AND c.wm_yr_wk = p.wm_yr_wk\n",
    "WHERE s.sales IS NOT NULL\n",
    "''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce85dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing evt_* columns and filling missing values...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n",
      "Processing sales_data chunk...\n"
     ]
    }
   ],
   "source": [
    "# Đọc sales_data để tạo cột evt_* và điền giá trị thiếu\n",
    "print(\"Processing evt_* columns and filling missing values...\")\n",
    "# Đọc sales_data theo từng chunk để tránh MemoryError\n",
    "chunk_size = 1000000  # Số hàng mỗi chunk\n",
    "first_chunk = True\n",
    "for chunk in pd.read_sql_query('SELECT * FROM sales_data', conn, chunksize=chunk_size):\n",
    "    print(f\"Processing sales_data chunk...\")\n",
    "\n",
    "    # Kết hợp sự kiện từ event_name_1 và event_name_2\n",
    "    chunk['events'] = chunk[['event_name_1', 'event_name_2']].apply(\n",
    "        lambda x: set([e for e in x if pd.notnull(e)]), axis=1\n",
    "    )\n",
    "    chunk['events'] = chunk['events'].apply(lambda x: list(x) if x else ['None'])\n",
    "\n",
    "    # Mã hóa one-hot cho các sự kiện\n",
    "    event_dummies = pd.get_dummies(chunk['events'].explode(), prefix='evt').groupby(level=0).sum()\n",
    "    chunk = pd.concat([chunk, event_dummies], axis=1)\n",
    "    chunk = chunk.drop(columns=['event_name_1', 'event_name_2', 'events'])\n",
    "\n",
    "    # Điền giá trị thiếu cho sell_price\n",
    "    chunk['sell_price'] = chunk['sell_price'].fillna(\n",
    "        chunk.groupby(['item_id', 'store_id'])['sell_price'].transform('mean')\n",
    "    )\n",
    "\n",
    "    # Lưu chunk vào SQLite\n",
    "    if first_chunk:\n",
    "        chunk.to_sql('sales_data_final', conn, if_exists='replace', index=False)\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_sql('sales_data_final', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6592a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu dữ liệu M5 vào database/m5_forecasting.db\n"
     ]
    }
   ],
   "source": [
    "# Xóa bảng sales_data tạm\n",
    "cursor.execute('DROP TABLE IF EXISTS sales_data')\n",
    "cursor.execute('ALTER TABLE sales_data_final RENAME TO sales_data')\n",
    "conn.commit()\n",
    "\n",
    "conn.close()\n",
    "print(\"Đã lưu dữ liệu M5 vào database/m5_forecasting.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
